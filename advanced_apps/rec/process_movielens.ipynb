{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We use the MoiveLens dataset for demonstration because it is commonly used for recommendation models. In this dataset, there are two types of nodes: users and movies. The movie nodes have three attributes: year, title and genre. There are ratings between user nodes and movie nodes. Each rating has a timestamp. In our recommendation model, we don't consider ratings and timestamps.\n",
    "\n",
    "**Note**: It is not necessarily the best dataset to demonstrate the power of GNN for recommendation. We have prepared the dataset to simplify the demonstration.\n",
    "\n",
    "To run the data preprocessing script, a user needs to download the English dictionary of the stanfordnlp package first. However, the following command only needs to run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please uncomment the two commands when the tutorial is run for the first time.\n",
    "#import stanfordnlp\n",
    "#stanfordnlp.download('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from movielens import MovieLens\n",
    "data = MovieLens('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate some statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = data.ratings\n",
    "user_id = np.array(ratings['user_idx'])\n",
    "movie_id = np.array(ratings['movie_idx'])\n",
    "user_movie_spm = spsp.coo_matrix((np.ones((len(user_id),)), (user_id, movie_id)))\n",
    "num_users, num_movies = user_movie_spm.shape\n",
    "print('#user-movie iterations:', len(movie_id))\n",
    "print('#users:', num_users)\n",
    "print('#movies:', num_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training, validation and testing sets. In the validation and testing dataset, each user has an item to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_test(user_movie_spm):\n",
    "    users = user_movie_spm.row\n",
    "    movies = user_movie_spm.col\n",
    "    picks = np.zeros(shape=(len(users)))\n",
    "    user_movie_spm = user_movie_spm.tocsr()\n",
    "    indptr = user_movie_spm.indptr\n",
    "    valid_set = np.zeros(shape=(num_users))\n",
    "    test_set = np.zeros(shape=(num_users))\n",
    "    for i in range(user_movie_spm.shape[0]):\n",
    "        start_idx = indptr[i]\n",
    "        end_idx = indptr[i+1]\n",
    "        idx = np.random.choice(np.arange(start_idx, end_idx), 2, replace=False)\n",
    "        valid_set[i] = movies[idx[0]]\n",
    "        picks[idx[0]] = 1\n",
    "        test_set[i] = movies[idx[1]]\n",
    "        picks[idx[1]] = 1\n",
    "    users = users[picks == 0]\n",
    "    movies = movies[picks == 0]\n",
    "    return spsp.coo_matrix((np.ones((len(users),)), (users, movies))), valid_set, test_set\n",
    "\n",
    "orig_user_movie_spm = user_movie_spm.tocsr()\n",
    "user_movie_spm, valid_set, test_set = pick_test(user_movie_spm)\n",
    "print('#training size:', user_movie_spm.nnz)\n",
    "users_valid = np.arange(num_users)\n",
    "movies_valid = valid_set\n",
    "users_test = np.arange(num_users)\n",
    "movies_test = test_set\n",
    "valid_size = len(users_valid)\n",
    "test_size = len(users_test)\n",
    "print('valid set:', valid_size)\n",
    "print('test set:', test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data split in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo = user_movie_spm.tocoo()\n",
    "train_map = {}\n",
    "valid_map = {}\n",
    "test_map = {}\n",
    "#print the training set.\n",
    "with open(\"train.txt\",\"w\") as file:\n",
    "    for row, col in zip(coo.row, coo.col):\n",
    "        train_map[(row, col)] = 1\n",
    "        file.write(str(row) + ', ' + str(col) + '\\n')\n",
    "    file.close()\n",
    "with open('valid.txt', 'w') as file:\n",
    "    for row, col in enumerate(valid_set):\n",
    "        valid_map[(row, col)] = 1\n",
    "        file.write(str(row) + ', ' + str(int(col)) + '\\n')\n",
    "    file.close()\n",
    "with open('test.txt', 'w') as file:\n",
    "    for row, col in enumerate(test_set):\n",
    "        test_map[(row, col)] = 1\n",
    "        file.write(str(row) + ', ' + str(int(col)) + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the negative samples are actually positive. Here we try to remove all of the postive ones from the negative set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_neg_set(user_movie_spm, neg_sample_size):\n",
    "    num_users = user_movie_spm.shape[0]\n",
    "    num_movies = user_movie_spm.shape[1]\n",
    "    neg_mat = np.zeros(shape=(num_users, neg_sample_size))\n",
    "    for user in range(num_users):\n",
    "        movie_set = set()\n",
    "        while len(movie_set) < neg_sample_size:\n",
    "            movies = np.random.choice(num_movies, neg_sample_size, replace=False)\n",
    "            for movie in movies:\n",
    "                if user_movie_spm[user, movie] == 0:\n",
    "                    movie_set.add(movie)\n",
    "                if len(movie_set) == neg_sample_size:\n",
    "                    break\n",
    "        neg_mat[user] = np.array(list(movie_set))\n",
    "\n",
    "    for user, movies in enumerate(neg_mat):\n",
    "        for idx, movie in enumerate(movies):\n",
    "            assert user_movie_spm[user, movie] == 0\n",
    "                \n",
    "    return neg_mat\n",
    "\n",
    "neg_valid = gen_neg_set(orig_user_movie_spm.tocsr(), 99)\n",
    "neg_test = gen_neg_set(orig_user_movie_spm.tocsr(), 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the negative sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('neg_valid.txt', 'w') as file:\n",
    "    for row, cols in enumerate(neg_valid):\n",
    "        for col in cols:\n",
    "            assert (row, col) not in train_map\n",
    "            assert (row, col) not in valid_map\n",
    "            assert (row, col) not in test_map\n",
    "            file.write(str(row) + ', ' + str(int(col)) + '\\n')\n",
    "    file.close()\n",
    "\n",
    "with open('neg_test.txt', 'w') as file:\n",
    "    for row, cols in enumerate(neg_test):\n",
    "        for col in cols:\n",
    "            assert (row, col) not in train_map\n",
    "            assert (row, col) not in valid_map\n",
    "            assert (row, col) not in test_map\n",
    "            file.write(str(row) + ', ' + str(int(col)) + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the item features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = np.expand_dims(data.movie_data['year'], axis=1)\n",
    "genre = data.movie_data['genre']\n",
    "title = data.movie_data['title']\n",
    "features = torch.tensor(np.concatenate((genre, title), axis=1), dtype=torch.float32)\n",
    "print('#features:', features.shape[1])\n",
    "in_feats = features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save everything in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(user_movie_spm, open('movielens_orig_train.pkl', 'wb'))\n",
    "pickle.dump(g, open('movielens_graph.pkl', 'wb'))\n",
    "pickle.dump(features, open('movielens_features.pkl', 'wb'))\n",
    "pickle.dump((valid_set, test_set), open('movielens_eval.pkl', 'wb'))\n",
    "pickle.dump((neg_valid, neg_test), open('movielens_neg.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
