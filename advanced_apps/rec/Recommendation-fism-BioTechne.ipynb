{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem setting\n",
    "\n",
    "In this tutorial, we demonstrate how graph neural networks can be used for recommendation. Here we focus on item-based recommendation model. This method in this tutorial recommends items that are similar to the ones purchased by the user. We demonstrate the recommendation model on the MovieLens dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started\n",
    "\n",
    "DGL can be used with different deep learning frameworks. Currently, DGL can be used with Pytorch and MXNet. Here, we show how DGL works with Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we load DGL, we need to set the DGL backend for one of the deep learning frameworks. Because this tutorial develops models in Pytorch, we have to set the DGL backend to Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "\n",
    "# Load Pytorch as backend\n",
    "dgl.load_backend('pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the rest of necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy import sparse as spsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bio-Techne data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627444\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "item_data = pickle.load(open('BioTechne/item_data.pkl', 'rb'))\n",
    "ratings = pickle.load(open('BioTechne/ratings.pkl', 'rb'))\n",
    "item_id_map = pickle.load(open('BioTechne/item_id_map.pkl', 'rb'))\n",
    "print(len(ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim data (optional)\n",
    "\n",
    "The original dataset has many long sessions. These sessions contain orders for different people. Thus, the original dataset is very noisy and confuses the FISM model to make right recommendation.\n",
    "\n",
    "One way to reduce noise in the dataset is to trim the original dataset and only keep the orders in the last hour in each session. Only keeping the orders in the last hour may make sessions less noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "ratings = ratings.sort_values(['user_idx', 'timestamp'])\n",
    "def trim_on_time(df):\n",
    "    oldest_time = np.max(df['timestamp']) - datetime.timedelta(seconds=3600)\n",
    "    return df[df['timestamp'] > oldest_time]\n",
    "ratings = ratings.groupby('user_idx').apply(trim_on_time)\n",
    "ratings = ratings.drop_duplicates(['user_idx', 'item_idx'])\n",
    "ratings['sess_id'] = ratings['user_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split long sessions (optional)\n",
    "\n",
    "Another option is to split the long sessions into short ones. The criteria of splitting the dataset is: if two consecutive orders are placed within 10 minutes, we consider them in the same session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "ratings = ratings.sort_values(['user_idx', 'timestamp'])\n",
    "def split_on_time(df):\n",
    "    time1 = np.array(df['timestamp'][1:], dtype=np.int64)\n",
    "    time2 = np.array(df['timestamp'][:-1], dtype=np.int64)\n",
    "    assignment = np.cumsum((time1 - time2)/1000000000 > 600)\n",
    "    assignment = np.concatenate([np.array([0], dtype=np.int64), assignment])\n",
    "    assert len(df) == len(assignment)\n",
    "    df['assign'] = assignment\n",
    "    return df\n",
    "# This is to split the orders within a session into subsessions based on the timestamp.\n",
    "# If two contiguous orders are placed within less than 30 minutes, they are considered within the same subsession.\n",
    "ratings = ratings.groupby('user_idx').apply(split_on_time)\n",
    "# drop the duplicated items within a subsession.\n",
    "ratings = ratings.drop_duplicates(['user_idx', 'assign', 'item_idx'])\n",
    "# Remove the subsessions that have fewer than 2 orders.\n",
    "ratings = ratings.groupby(['user_idx', 'assign']).filter(lambda df: len(df) > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#original users: 14624\n",
      "#new users: 31132\n"
     ]
    }
   ],
   "source": [
    "print('#original users:', np.max(ratings['user_idx']) + 1)\n",
    "# a combination of 'user_idx' and 'assign' defines a new user\n",
    "num_users = len(ratings.groupby(['user_idx', 'assign']))\n",
    "print('#new users:', num_users)\n",
    "user_idx = ratings['user_idx']\n",
    "assign = ratings['assign']\n",
    "num_groups = np.max(assign) + 1\n",
    "# The session id identifies a unique subsession.\n",
    "ratings['sess_id'] = user_idx * num_groups + assign\n",
    "assert num_users == len(np.unique(ratings['sess_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31131\n"
     ]
    }
   ],
   "source": [
    "uniq_sess = np.unique(ratings['sess_id'])\n",
    "sess_map = {sess_id: i for i, sess_id in enumerate(uniq_sess)}\n",
    "ratings['sess_id'] = ratings['sess_id'].map(sess_map)\n",
    "print(np.max(ratings['sess_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data to run models\n",
    "\n",
    "Here we split orders and place them in three sets: training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_idx = ratings['sess_id']\n",
    "user_item_spm = spsp.coo_matrix((np.ones(len(ratings)), (user_idx, ratings['item_idx'])))\n",
    "features = np.concatenate([item_data['title'], item_data['item_types']], 1)\n",
    "\n",
    "user_span = np.zeros(user_item_spm.shape[0])\n",
    "for i in range(user_item_spm.shape[0]):\n",
    "    user_rating = ratings[user_idx == i]\n",
    "    if len(user_rating) == 0:\n",
    "        continue\n",
    "    timestamp = np.array(user_rating['timestamp'])\n",
    "    start = np.min(timestamp)\n",
    "    end = np.max(timestamp)\n",
    "    span = int(end - start)/1000000000\n",
    "    user_span[i] = span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can potentially use the embeddings computed from the knowledge graph. For now, let's not use the knowledge graph embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "kge_feats = pickle.load(open('BioTechne/bio-techne_entity_embed_features.pkl', 'rb'))\n",
    "item_kge_map = pd.read_csv('BioTechne/_matched_item2feaidx', sep='\\t', header=None)\n",
    "item_kge_map = {item_name: kge_idx for kge_idx, item_name in zip(item_kge_map[0], item_kge_map[1])}\n",
    "print(len(item_kge_map))\n",
    "\n",
    "orig_idx2_kge_idx = {}\n",
    "for item_id, item_idx in zip(ratings['item_id'], ratings['item_idx']):\n",
    "    if item_id in item_kge_map:\n",
    "        orig_idx2_kge_idx[item_idx] = item_kge_map[item_id]\n",
    "\n",
    "exist_mask = np.zeros((len(ratings['item_id'])))\n",
    "for i, item_idx in enumerate(ratings['item_idx']):\n",
    "    if item_idx in orig_idx2_kge_idx:\n",
    "        exist_mask[i] = 1\n",
    "exist_mask = (exist_mask == 1)\n",
    "ratings = ratings[exist_mask]\n",
    "\n",
    "item_feats = np.zeros((len(orig_idx2_kge_idx), features.shape[1]), dtype=np.float32)\n",
    "for orig_idx, kge_idx in orig_idx2_kge_idx.items():\n",
    "    item_feats[kge_idx] = features[orig_idx]\n",
    "\n",
    "#features = kge_feats\n",
    "print(kge_feats.dtype)\n",
    "print(item_feats.dtype)\n",
    "features = item_feats\n",
    "print(features.shape)\n",
    "\n",
    "user_item_spm = spsp.coo_matrix((np.ones(len(ratings)), (ratings['user_idx'], ratings['item_idx'])))\n",
    "user_idx = user_item_spm.row\n",
    "item_idx = user_item_spm.col\n",
    "for i, orig_idx in enumerate(item_idx):\n",
    "    if orig_idx in orig_idx2_kge_idx:\n",
    "        item_idx[i] = orig_idx2_kge_idx[orig_idx]\n",
    "    else:\n",
    "        item_idx[i] = -1\n",
    "user_idx = user_idx[item_idx >= 0]\n",
    "item_idx = item_idx[item_idx >= 0]\n",
    "user_item_spm = spsp.coo_matrix((np.ones(len(user_idx)), (user_idx, item_idx))).tocsr()\n",
    "print(user_item_spm.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31132, 3256)\n",
      "0\n",
      "0\n",
      "(31132, 3256)\n"
     ]
    }
   ],
   "source": [
    "print(user_item_spm.shape)\n",
    "num_items = user_item_spm.shape[1]\n",
    "user_deg = user_item_spm.dot(np.ones((num_items)))\n",
    "print(np.sum(user_deg == 1))\n",
    "print(np.sum(user_deg == 2))\n",
    "user_item_spm = user_item_spm.tocsr()\n",
    "user_item_spm = user_item_spm[np.nonzero(user_deg > 2)]\n",
    "user_span = user_span[np.nonzero(user_deg > 2)]\n",
    "print(user_item_spm.shape)\n",
    "num_users = user_item_spm.shape[0]\n",
    "num_items = user_item_spm.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training, validation and testing. For each user/session, we randomly pick an item from this user as the validation item. Similarly, we randomly pick an item as the test item. We use the remaining items as the training set for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training size: 117267\n",
      "valid set: 31132\n",
      "test set: 31132\n"
     ]
    }
   ],
   "source": [
    "def pick_test(user_item_spm):\n",
    "    user_item_spm = user_item_spm.tocoo()\n",
    "    users = user_item_spm.row\n",
    "    items = user_item_spm.col\n",
    "    picks = np.zeros(shape=(len(users)))\n",
    "    user_item_spm = user_item_spm.tocsr()\n",
    "    indptr = user_item_spm.indptr\n",
    "    valid_set = np.zeros(shape=(num_users))\n",
    "    test_set = np.zeros(shape=(num_users))\n",
    "    for i in range(user_item_spm.shape[0]):\n",
    "        start_idx = indptr[i]\n",
    "        end_idx = indptr[i+1]\n",
    "        idx = np.random.choice(np.arange(start_idx, end_idx), 2, replace=False)\n",
    "        valid_set[i] = items[idx[0]]\n",
    "        picks[idx[0]] = 1\n",
    "        test_set[i] = items[idx[1]]\n",
    "        picks[idx[1]] = 1\n",
    "    users = users[picks == 0]\n",
    "    items = items[picks == 0]\n",
    "    return spsp.coo_matrix((np.ones((len(users),)), (users, items))), valid_set, test_set\n",
    "\n",
    "np.random.seed(0)\n",
    "orig_user_item_spm = user_item_spm.tocsr()\n",
    "user_item_spm, valid_set, test_set = pick_test(user_item_spm)\n",
    "print('#training size:', user_item_spm.nnz)\n",
    "users_valid = np.arange(num_users)\n",
    "items_valid = valid_set\n",
    "users_test = np.arange(num_users)\n",
    "items_test = test_set\n",
    "valid_size = len(users_valid)\n",
    "test_size = len(users_test)\n",
    "num_users = user_item_spm.shape[0]\n",
    "num_items = user_item_spm.shape[1]\n",
    "print('valid set:', valid_size)\n",
    "print('test set:', test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we compute SVD on the training dataset to generate more item embeddings. Here we compute 100 singular vectors for each item and concatenate them with items' original features (titles and item types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vt = spsp.linalg.svds(user_item_spm, k=100)\n",
    "v = vt.transpose() * np.sqrt(s).transpose()\n",
    "features = np.concatenate((item_data['title'], item_data['item_types'], v), 1).astype(np.float32)\n",
    "in_feats = features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31132, 3256)\n",
      "18086\n"
     ]
    }
   ],
   "source": [
    "user_deg = user_item_spm.dot(np.ones((num_items)))\n",
    "print(user_item_spm.shape)\n",
    "print(np.sum(user_deg < 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data into a file so that we can make a fair comparion with other recommendation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_user_item_spm = orig_user_item_spm.tocoo()\n",
    "orig_users = orig_user_item_spm.row\n",
    "orig_items = orig_user_item_spm.col\n",
    "valid_set = set()\n",
    "test_set = set()\n",
    "for user, item in zip(users_valid, items_valid):\n",
    "    valid_set.add((user, item))\n",
    "for user, item in zip(users_test, items_test):\n",
    "    test_set.add((user, item))\n",
    "valid_mask = np.zeros((len(orig_users)), dtype=np.int64)\n",
    "test_mask = np.zeros((len(orig_users)), dtype=np.int64)\n",
    "for i in range(len(valid_mask)):\n",
    "    user = orig_users[i]\n",
    "    item = orig_items[i]\n",
    "    if (user, item) in valid_set:\n",
    "        valid_mask[i] = 1\n",
    "    elif (user, item) in test_set:\n",
    "        test_mask[i] = 1\n",
    "assert np.sum(valid_mask) == num_users\n",
    "assert np.sum(test_mask) == num_users\n",
    "full_data = np.concatenate([np.expand_dims(orig_users, 1),\n",
    "                            np.expand_dims(orig_items, 1),\n",
    "                            np.expand_dims(valid_mask, 1),\n",
    "                            np.expand_dims(test_mask, 1)], 1)\n",
    "np.savetxt('bio-techne-split-sess.csv', full_data, fmt='%d', delimiter=',')\n",
    "\n",
    "# verify\n",
    "test_spm = user_item_spm.tocsr()\n",
    "for i in range(len(full_data)):\n",
    "    user = full_data[i, 0]\n",
    "    item = full_data[i, 1]\n",
    "    if full_data[i, 2] == 1:\n",
    "        assert item == items_valid[user]\n",
    "    elif full_data[i, 3] == 1:\n",
    "        assert item == items_test[user]\n",
    "    else:\n",
    "        assert test_spm[user, item] != 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The recommendation model\n",
    "\n",
    "At large, the model first learns item embeddings from the user-item interaction dataset and use the item embeddings to recommend users similar items they have purchased. To learn item embeddings, we first need to construct an item similarity graph and train GNN on the item graph.\n",
    "\n",
    "There are many ways of constructing the item similarity graph. Here we use the [SLIM model](https://dl.acm.org/citation.cfm?id=2118303) to learn item similarity and use the learned result to construct the item graph. The resulting graph will have an edge between two items if they are similar and the edge has a weight that represents the similarity score.\n",
    "\n",
    "After the item similarity graph is constructed, we run a GNN model on it and use the vertex connectivity as the training signal to train the GNN model. The GNN training procedure is very similar to the link prediction task in [the previous section](https://github.com/zheng-da/DGL_devday_tutorial/blob/master/BasicTasks_pytorch.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the item graph with SLIM\n",
    "SLIM is an item-based recommendation model. When training SLIM on a user-item dataset, it learns an item similarity graph. This similarity graph is the item graph we construct for the GNN model.\n",
    "\n",
    "Please follow the instruction on the [SLIM github repo](https://github.com/KarypisLab/SLIM) to install SLIM.\n",
    "\n",
    "To use SLIM to generate an item similarity graph, there are two hyperparameters we can tune. `l1r` is the co-efficient for the L1 regularization and `l2r` is the co-efficient for the L2 regularization. Increasing `l1r` will generate a sparser similarity graph and increasing `l2r` leads to a denser similarity graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_construct import create_SLIM_graph\n",
    "item_spm = create_SLIM_graph(user_item_spm, l1r=1, l2r=1, test=False)\n",
    "use_edge_similarity = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk=30\n",
    "dense_item = np.sort(item_spm.todense())\n",
    "topk_item = dense_item[:,-topk]\n",
    "topk_item_spm = item_spm > topk_item\n",
    "topk_item_spm = spsp.csr_matrix(topk_item_spm)\n",
    "item_spm = item_spm.multiply(topk_item_spm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = item_spm.dot(np.ones((num_items)))\n",
    "print(item_spm.nnz)\n",
    "print(np.sum(deg == 0))\n",
    "print(len(deg))\n",
    "print(item_spm.sum(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the co-occurence graph\n",
    "Or we can simply construct a co-occurrence graph. That is, if two items are used by the same user, we draw an edge between these two items.\n",
    "\n",
    "When using this method for graph construction, there are also two hyperparameters to tune. `downsample_factor` controls how much we should down sample user-item pairs based on the frequency of items. A larger `downsample_factor` leads more down sampling. `topk` controls how many items should an item connect to. If it's None, an item connects to all items that have co-occurrence with the item; otherwise, an item connects with the most frequently co-occurred items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_construct import create_cooccur_graph\n",
    "item_spm = create_cooccur_graph(user_item_spm, downsample_factor=1e-5, topk=50)\n",
    "use_edge_similarity = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the cosine-similarity graph\n",
    "We can also use cosine similarity to build a graph. We compute the cosine similarity of the neighborhoods of every pair of items. This is quite similar to co-occurrence graph except that we use cosine similarity instead of the number of co-occurrence to measure the similarity of two items.\n",
    "\n",
    "In this case, there is one hyperparameter `topk`. If it's specified, an item connects to top K most similar items in terms of cosine similarity in the neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_construct import create_cosine_graph\n",
    "item_spm = create_cosine_graph(user_item_spm, topk=10)\n",
    "use_edge_similarity = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we construct the graph, we load it to the DGL graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nodes: 3256\n",
      "#edges: 27759\n"
     ]
    }
   ],
   "source": [
    "g = dgl.DGLGraph(item_spm, readonly=True)\n",
    "g.edata['similarity'] = torch.tensor(item_spm.data, dtype=torch.float32)\n",
    "g.ndata['feats'] = torch.tensor(features)\n",
    "#g.ndata['id'] = torch.arange(num_items, dtype=torch.int64)\n",
    "print('#nodes:', g.number_of_nodes())\n",
    "print('#edges:', g.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN models\n",
    "\n",
    "We run GNN on the item graph to compute item embeddings. In this tutorial, we use a customized [GraphSage](https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf) model to compute node embeddings. The original GraphSage performs the following computation on every node $v$ in the graph:\n",
    "\n",
    "$$h_{N(v)}^{(l)} \\gets AGGREGATE_k({h_u^{(l-1)}, \\forall u \\in N(v)})$$\n",
    "$$h_v^{(l)} \\gets \\sigma(W^k \\cdot CONCAT(h_v^{(l-1)}, h_{N(v)}^{(l)})),$$\n",
    "\n",
    "where $N(v)$ is the neighborhood of node $v$ and $l$ is the layer Id.\n",
    "\n",
    "The original GraphSage model treats each neighbor equally. However, the SLIM model learns the item similarity based on the user-item iteration. The GNN model should take the similarity into account. Thus, we customize the GraphSage model in the following fashion. Instead of aggregating all neighbors equally, we aggregate neighbors embeddings rescaled by the similarity on the edges. Thus, the aggregation step is defined as follows:\n",
    "\n",
    "$$h_{N(v)}^{(l)} \\gets \\Sigma_{u \\in N(v)}({h_u^{(l-1)} * s_{uv}}),$$\n",
    "\n",
    "where $s_{uv}$ is the similarity score between two vertices $u$ and $v$.\n",
    "\n",
    "The GNN model has multiple layers. In each layer, a vertex accesses its direct neighbors. When we stack $k$ layers in a model, a node $v$ access neighbors within $k$ hops. The output of the GNN model is node embeddings that represent the nodes and all information in the k-hop neighborhood.\n",
    "\n",
    "<img src=\"https://github.com/zheng-da/DGL_devday_tutorial/raw/master/GNN.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "We implement the computation in each layer of the customized GraphSage model in `SAGEConv` and implement the multi-layer model in `GraphSAGEModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_edge_similarity:\n",
    "    from sageconv import SAGEConv\n",
    "else:\n",
    "    from dgl.nn.pytorch.conv import SAGEConv\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 out_dim,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 aggregator_type):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.norm = nn.LayerNorm((out_dim,))\n",
    "        self.layers = nn.ModuleList()\n",
    "        if n_layers == 1:\n",
    "            self.layers.append(SAGEConv(in_feats, out_dim, aggregator_type,\n",
    "                                        feat_drop=dropout, activation=None))\n",
    "        elif n_layers > 1:\n",
    "            # input layer\n",
    "            self.layers.append(SAGEConv(in_feats, n_hidden, aggregator_type,\n",
    "                                        feat_drop=dropout, activation=activation))\n",
    "            # hidden layer\n",
    "            for i in range(n_layers - 2):\n",
    "                self.layers.append(SAGEConv(n_hidden, n_hidden, aggregator_type,\n",
    "                                            feat_drop=dropout, activation=activation))\n",
    "            # output layer\n",
    "            self.layers.append(SAGEConv(n_hidden, out_dim, aggregator_type,\n",
    "                                        feat_drop=dropout, activation=None))\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            if use_edge_similarity:\n",
    "                h = layer(g, h, g.edata['similarity'])\n",
    "            else:\n",
    "                h = layer(g, h)\n",
    "            #h = tmp + prev_h\n",
    "            #prev_h = h\n",
    "        h = self.norm(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Item Embeddings\n",
    "\n",
    "We train the item embeddings with the edges in the item graph as the training signal. This step is very similar to the link prediction task in the [basic applications](https://github.com/zheng-da/DGL_devday_tutorial/blob/master/BasicTasks_pytorch.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the MovieLens dataset has sparse features (both genre and title are stored as multi-hot encoding). The sparse features have many dimensions. To run GNN on the item features, we first create an encoding layer to project the sparse features to a lower dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_embeddings(h, ndata, emb, proj):\n",
    "    '''Combine node-specific trainable embedding ``h`` with categorical inputs\n",
    "    (projected by ``emb``) and numeric inputs (projected by ``proj``).\n",
    "    '''\n",
    "    e = []\n",
    "    for key, value in ndata.items():\n",
    "        if value.dtype == torch.int64:\n",
    "            e.append(emb[key](value))\n",
    "        elif value.dtype == torch.float32:\n",
    "            e.append(proj[key](value))\n",
    "    if len(e) == 0:\n",
    "        return h\n",
    "    else:\n",
    "        return h + torch.stack(e, 0).sum(0)\n",
    "    \n",
    "class EncodeLayer(nn.Module):\n",
    "    def __init__(self, ndata, num_hidden, device):\n",
    "        super(EncodeLayer, self).__init__()\n",
    "        self.proj = nn.ModuleDict()\n",
    "        self.emb = nn.ModuleDict()\n",
    "        for key in ndata.keys():\n",
    "            vals = ndata[key]\n",
    "            if vals.dtype == torch.float32:\n",
    "                self.proj[key] = nn.Linear(ndata[key].shape[1], num_hidden)\n",
    "                #self.proj[key] = nn.Sequential(\n",
    "                #                    nn.Linear(ndata[key].shape[1], num_hidden),\n",
    "                #                    nn.LeakyReLU(),\n",
    "                #                    )\n",
    "            elif vals.dtype == torch.int64:\n",
    "                self.emb[key] = nn.Embedding(\n",
    "                            vals.max().item() + 1,\n",
    "                            num_hidden,\n",
    "                            padding_idx=0)\n",
    "                \n",
    "    def forward(self, ndata):\n",
    "        return mix_embeddings(0, ndata, self.emb, self.proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FISMrating(nn.Module):\n",
    "    r\"\"\"\n",
    "    PinSAGE + FISM for item-based recommender systems\n",
    "    The formulation of FISM goes as\n",
    "    .. math::\n",
    "       r_{ui} = b_u + b_i + \\left(n_u^+\\right)^{-\\alpha}\n",
    "       \\sum_{j \\in R_u^+} p_j q_i^\\top\n",
    "    In FISM, both :math:`p_j` and :math:`q_i` are trainable parameters.  Here\n",
    "    we replace them as outputs from two PinSAGE models ``P`` and\n",
    "    ``Q``.\n",
    "    \"\"\"\n",
    "    def __init__(self, P, Q, num_users, num_movies, alpha=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.P = P\n",
    "        self.Q = Q\n",
    "        self.b_u = nn.Parameter(torch.zeros(num_users))\n",
    "        self.b_i = nn.Parameter(torch.zeros(num_movies))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    \n",
    "    def forward(self, I, U, I_neg, I_U, N_U, test):\n",
    "        '''\n",
    "        I: 1D LongTensor\n",
    "        U: 1D LongTensor\n",
    "        I_neg: 2D LongTensor (batch_size, n_negs)\n",
    "        '''\n",
    "        batch_size = I.shape[0]\n",
    "        device = I.device\n",
    "        I_U = I_U.to(device)\n",
    "        # number of interacted items\n",
    "        N_U = N_U.to(device)\n",
    "        U_idx = torch.arange(U.shape[0], device=device).repeat_interleave(N_U)\n",
    "\n",
    "        q = self.Q(I)\n",
    "        p = self.P(I_U)\n",
    "        # If this is training, we need to subtract the embedding of the self node from the context embedding\n",
    "        if not test:\n",
    "            p_self = self.P(I)\n",
    "        p_sum = torch.zeros_like(q)\n",
    "        p_sum = p_sum.scatter_add(0, U_idx[:, None].expand_as(p), p)    # batch_size, n_dims\n",
    "        if test:\n",
    "            p_ctx = p_sum\n",
    "            pq = (p_ctx * q).sum(1) / (N_U.float() ** self.alpha)\n",
    "        else:\n",
    "            p_ctx = p_sum - p_self\n",
    "            pq = (p_ctx * q).sum(1) / ((N_U.float() - 1).clamp(min=1) ** self.alpha)\n",
    "        r = self.b_u[U] + self.b_i[I] + pq\n",
    "\n",
    "        if I_neg is not None:\n",
    "            n_negs = I_neg.shape[1]\n",
    "            I_neg_flat = I_neg.view(-1)\n",
    "            q_neg = self.Q(I_neg_flat)\n",
    "            q_neg = q_neg.view(batch_size, n_negs, -1)  # batch_size, n_negs, n_dims\n",
    "            if test:\n",
    "                pq_neg = (p_ctx.unsqueeze(1) * q_neg).sum(2) / (N_U.float().unsqueeze(1) ** self.alpha)\n",
    "            else:\n",
    "                pq_neg = (p_ctx.unsqueeze(1) * q_neg).sum(2) / ((N_U.float() - 1).clamp(min=1).unsqueeze(1) ** self.alpha)\n",
    "            r_neg = self.b_u[U].unsqueeze(1) + self.b_i[I_neg] + pq_neg\n",
    "            return r, r_neg\n",
    "        else:\n",
    "            return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the FISM model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0\n",
    "gamma = 0\n",
    "\n",
    "def rank_loss2(pos_score, neg_score, true_neg):\n",
    "    pos_score = torch.unsqueeze(pos_score, 1)\n",
    "    return torch.sum(torch.mul(F.logsigmoid(pos_score - neg_score), true_neg)) * (-1.0)\n",
    "    #return torch.sum(F.logsigmoid(pos_score - neg_score)) * (-1.0)\n",
    "\n",
    "\n",
    "class FISM(nn.Module):\n",
    "    def __init__(self, user_item_spm, gconv_p, gconv_q, g, num_hidden, device):\n",
    "        super(FISM, self).__init__()\n",
    "        num_users = user_item_spm.shape[0]\n",
    "        num_movies = user_item_spm.shape[1]\n",
    "        self.encode_p = EncodeLayer(g.ndata, num_hidden, device)\n",
    "        self.encode_q = EncodeLayer(g.ndata, num_hidden, device)\n",
    "        self.gconv_p = gconv_p\n",
    "        self.gconv_q = gconv_q\n",
    "        P = lambda I: self.gconv_p(g, self.encode_p(g.ndata))[I]\n",
    "        Q = lambda I: self.gconv_q(g, self.encode_q(g.ndata))[I]\n",
    "        self.fism_rating = FISMrating(P, Q, num_users, num_movies, 1)\n",
    "\n",
    "    def est_rating(self, I, U, I_neg, I_U, N_U):\n",
    "        r, r_neg = self.fism_rating(I, U, I_neg, I_U, N_U, True)\n",
    "        neg_sample_size = int(len(r_neg) / len(r))\n",
    "        return torch.unsqueeze(r, 1), r_neg.reshape((-1, neg_sample_size))\n",
    "\n",
    "    def loss(self, r_ui, neg_r_ui, true_neg):\n",
    "        return rank_loss2(r_ui, neg_r_ui, true_neg)\n",
    "        #diff = 1 - (r_ui - neg_r_ui)\n",
    "        #return torch.sum(torch.mul(diff, diff)/2)# \\\n",
    "        #    + beta/2 * torch.sum(torch.mul(P, P) + torch.mul(Q, Q)) \\\n",
    "        #    + gamma/2 * (torch.sum(torch.mul(self.fism_rating.b_u, self.fism_rating.b_u)) \\\n",
    "        #                 + torch.sum(torch.mul(self.fism_rating.b_i, self.fism_rating.b_i)))\n",
    "\n",
    "    def forward(self, I, U, I_neg, true_neg, I_U, N_U):\n",
    "        r, r_neg = self.fism_rating(I, U, I_neg, I_U, N_U, False)\n",
    "        #neg_sample_size = int(len(r_neg) / len(r))\n",
    "        #r_neg = r_neg.reshape((-1, neg_sample_size))\n",
    "        return self.loss(r, r_neg, true_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeSampler:\n",
    "    def __init__(self, user_item_spm, batch_size, neg_sample_size):\n",
    "        edge_ids = np.random.permutation(user_item_spm.nnz)\n",
    "        self.batches = np.split(edge_ids, np.arange(batch_size, len(edge_ids), batch_size))\n",
    "        self.idx = 0\n",
    "        user_item_spm = user_item_spm.tocoo()\n",
    "        self.users = user_item_spm.row\n",
    "        self.movies = user_item_spm.col\n",
    "        self.user_item_spm = user_item_spm.tocsr()\n",
    "        self.num_movies = user_item_spm.shape[1]\n",
    "        self.num_users = user_item_spm.shape[0]\n",
    "        self.neg_sample_size = neg_sample_size\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.idx == len(self.batches):\n",
    "            raise StopIteration\n",
    "        batch = self.batches[self.idx]\n",
    "        self.idx += 1\n",
    "        I = self.movies[batch]\n",
    "        U = self.users[batch]\n",
    "        neighbors = self.user_item_spm[U]\n",
    "        I_neg = np.random.choice(num_items, self.neg_sample_size * len(batch)).reshape(-1, self.neg_sample_size)\n",
    "        true_neg = np.zeros_like(I_neg)\n",
    "        for i in range(self.neg_sample_size):\n",
    "            true_neg[:,i] = self.user_item_spm[U, I_neg[:,i]] == 0\n",
    "        I = torch.LongTensor(I).to(device)\n",
    "        U = torch.LongTensor(U).to(device)\n",
    "        I_neg = torch.LongTensor(I_neg).to(device)\n",
    "        I_U = torch.LongTensor(neighbors.indices).to(device)\n",
    "        N_U = torch.LongTensor(neighbors.indptr[1:] - neighbors.indptr[:-1]).to(device)\n",
    "        true_neg = torch.FloatTensor(true_neg).to(device)\n",
    "        return I, U, I_neg, true_neg, I_U, N_U\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the performance of the trained item embeddings in the item-based recommendation task. We use the last item that a user purchased to represent the user and compute the similarity between the last item and a list of items (an item the user will purchase and a set of randomly sampled items). We calculate the ranking of the item that will be purchased among the list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecEval(model, user_item_spm, k, users_eval, items_eval, neg_eval):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        neg_items_eval = neg_eval[users_eval]\n",
    "        neighbors = user_item_spm.tocsr()[users_eval]\n",
    "        I_U = torch.LongTensor(neighbors.indices)\n",
    "        N_U = torch.LongTensor(neighbors.indptr[1:] - neighbors.indptr[:-1])\n",
    "        r, neg_r = model.est_rating(torch.LongTensor(items_eval).to(device),\n",
    "                                    torch.LongTensor(users_eval).to(device),\n",
    "                                    torch.LongTensor(neg_items_eval).to(device),\n",
    "                                    I_U.to(device),\n",
    "                                    N_U.to(device))\n",
    "        neg_sample_size = int(len(neg_r) / len(r))\n",
    "        neg_r = neg_r.reshape((-1, neg_sample_size))\n",
    "        hits = (torch.sum(neg_r >= r, 1) <= k).cpu().numpy()\n",
    "        return np.mean(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_spm = user_item_spm.tocoo()\n",
    "all_users = []\n",
    "all_items = []\n",
    "all_users.append(user_item_spm.row)\n",
    "all_items.append(user_item_spm.col)\n",
    "all_users.append(users_valid)\n",
    "all_items.append(items_valid)\n",
    "all_users = np.concatenate(all_users).astype(np.int64)\n",
    "all_items = np.concatenate(all_items).astype(np.int64)\n",
    "all_spm1 = spsp.coo_matrix((np.ones((len(all_users))), (all_users, all_items)))\n",
    "\n",
    "all_users = []\n",
    "all_items = []\n",
    "all_users.append(user_item_spm.row)\n",
    "all_items.append(user_item_spm.col)\n",
    "all_users.append(users_valid)\n",
    "all_items.append(items_valid)\n",
    "all_users.append(users_test)\n",
    "all_items.append(items_test)\n",
    "all_users = np.concatenate(all_users).astype(np.int64)\n",
    "all_items = np.concatenate(all_items).astype(np.int64)\n",
    "all_spm2 = spsp.coo_matrix((np.ones((len(all_users))), (all_users, all_items)))\n",
    "        \n",
    "def RecEvalAll(model, user_item_spm, k, eval_type):\n",
    "    model.eval()\n",
    "    if eval_type == 'valid':\n",
    "        ctx_spm = user_item_spm.tocsr()\n",
    "        pos_spm = all_spm1.tocsr()\n",
    "        users_eval = users_valid\n",
    "        items_eval = items_valid\n",
    "    elif eval_type == 'test':\n",
    "        ctx_spm = all_spm1.tocsr()\n",
    "        pos_spm = all_spm2.tocsr()\n",
    "        users_eval = users_test\n",
    "        items_eval = items_test\n",
    "    else:\n",
    "        raise Exception()\n",
    "\n",
    "    batch_size = 1024\n",
    "    batches = np.split(np.arange(len(users_eval)), np.arange(batch_size, len(users_eval), batch_size))\n",
    "    all_hits = np.zeros((user_item_spm.shape[0],), dtype=np.int64)\n",
    "    with torch.no_grad():\n",
    "        hits_list = []\n",
    "        for idx in batches:\n",
    "            users = users_eval[idx]\n",
    "            items = items_eval[idx]\n",
    "            neg_items_eval = np.tile(np.arange(num_items), len(users)).reshape(len(users), num_items)\n",
    "            neigh_ctx = ctx_spm[users]\n",
    "            pos_neighbors = pos_spm[users]\n",
    "            assert neigh_ctx.nnz > 0\n",
    "            I_U = torch.LongTensor(neigh_ctx.indices)\n",
    "            N_U = torch.LongTensor(neigh_ctx.indptr[1:] - neigh_ctx.indptr[:-1])\n",
    "            r, neg_r = model.est_rating(torch.LongTensor(items).to(device),\n",
    "                                        torch.LongTensor(users).to(device),\n",
    "                                        torch.LongTensor(neg_items_eval).to(device),\n",
    "                                        I_U.to(device),\n",
    "                                        N_U.to(device))\n",
    "            neg_sample_size = num_items\n",
    "            # Here neg_r includes the scores on the positive edges. let's make the scores\n",
    "            # on the positive edges very small. This is equivalent to exclude positive edges\n",
    "            # from negative edges.\n",
    "            neg_r = neg_r.reshape((-1, neg_sample_size)).cpu().numpy() - pos_neighbors * 10\n",
    "            hits = (np.sum(neg_r > r.cpu().numpy(), 1) < k)\n",
    "            all_hits[idx] = np.squeeze(hits)\n",
    "            #hits_list.append(hits)\n",
    "    #ranges = [0, 60, 600, 1800, 3600, 3600*2, 3600*5, 3600*24]\n",
    "    #for i in range(len(ranges) - 1):\n",
    "    #    user_in_range = np.logical_and(user_span >= ranges[i], user_span < ranges[i+1])\n",
    "    #    if np.sum(user_in_range) > 0:\n",
    "    #        print('[{}-{})({})'.format(ranges[i], ranges[i+1], np.sum(user_in_range)),\n",
    "    #              np.mean(all_hits[user_in_range]))\n",
    "    return np.mean(all_hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put everything in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feats': tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -8.5190e-03,\n",
       "          7.1877e-03, -8.0836e-04],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -9.3564e-03,\n",
       "          9.2015e-03, -8.4092e-04],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.1970e-04,\n",
       "          4.8619e-03, -1.1785e-03],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.1188e-01,\n",
       "          1.2709e-01, -2.5882e-03],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.1160e-02,\n",
       "          6.2134e-02, -9.3057e-04],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.0177e+00,\n",
       "          1.0163e+00, -1.0442e-02]], device='cuda:0')}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#edges: 117267\n",
      "#batch/epoch: 114.5185546875\n",
      "Epoch 00000 | train 2.3398 | eval 2.1600 | Loss 20555.3052 | HITS@10 sub:0.0000 | HITS@10 all:0.0947\n",
      "test acc:0.1012\n",
      "Epoch 00001 | train 2.3359 | eval 2.1433 | Loss 10008.6790 | HITS@10 sub:0.0000 | HITS@10 all:0.1020\n",
      "test acc:0.1100\n",
      "Epoch 00002 | train 2.3366 | eval 2.1486 | Loss 9272.1274 | HITS@10 sub:0.0000 | HITS@10 all:0.1034\n",
      "test acc:0.1119\n",
      "Epoch 00003 | train 2.3346 | eval 2.1427 | Loss 8843.9131 | HITS@10 sub:0.0000 | HITS@10 all:0.1082\n",
      "test acc:0.1158\n",
      "Epoch 00004 | train 2.3393 | eval 2.1462 | Loss 8555.6296 | HITS@10 sub:0.0000 | HITS@10 all:0.1115\n",
      "test acc:0.1189\n",
      "Epoch 00005 | train 2.3384 | eval 2.1438 | Loss 8272.8355 | HITS@10 sub:0.0000 | HITS@10 all:0.1151\n",
      "test acc:0.1274\n",
      "Epoch 00006 | train 2.3377 | eval 2.1417 | Loss 8061.9247 | HITS@10 sub:0.0000 | HITS@10 all:0.1210\n",
      "test acc:0.1312\n",
      "Epoch 00007 | train 2.3380 | eval 2.1570 | Loss 7841.2546 | HITS@10 sub:0.0000 | HITS@10 all:0.1235\n",
      "test acc:0.1354\n",
      "Epoch 00008 | train 2.3383 | eval 2.1468 | Loss 7646.5939 | HITS@10 sub:0.0000 | HITS@10 all:0.1270\n",
      "test acc:0.1370\n",
      "Epoch 00009 | train 2.3339 | eval 2.1666 | Loss 7471.0760 | HITS@10 sub:0.0000 | HITS@10 all:0.1301\n",
      "test acc:0.1413\n",
      "Epoch 00010 | train 2.3341 | eval 2.1511 | Loss 7282.1329 | HITS@10 sub:0.0000 | HITS@10 all:0.1347\n",
      "test acc:0.1480\n",
      "Epoch 00011 | train 2.3348 | eval 2.1436 | Loss 7130.9618 | HITS@10 sub:0.0000 | HITS@10 all:0.1356\n",
      "test acc:0.1499\n",
      "Epoch 00012 | train 2.3354 | eval 2.1514 | Loss 6969.0782 | HITS@10 sub:0.0000 | HITS@10 all:0.1368\n",
      "test acc:0.1498\n",
      "Epoch 00013 | train 2.3340 | eval 2.1438 | Loss 6830.0136 | HITS@10 sub:0.0000 | HITS@10 all:0.1368\n",
      "Epoch 00014 | train 2.3406 | eval 2.1561 | Loss 6723.3724 | HITS@10 sub:0.0000 | HITS@10 all:0.1412\n",
      "test acc:0.1554\n",
      "Epoch 00015 | train 2.3351 | eval 2.1538 | Loss 6591.0593 | HITS@10 sub:0.0000 | HITS@10 all:0.1407\n",
      "Epoch 00016 | train 2.3898 | eval 2.1722 | Loss 6483.3992 | HITS@10 sub:0.0000 | HITS@10 all:0.1434\n",
      "test acc:0.1596\n",
      "Epoch 00017 | train 2.4000 | eval 2.1647 | Loss 6344.0322 | HITS@10 sub:0.0000 | HITS@10 all:0.1440\n",
      "test acc:0.1617\n",
      "Epoch 00018 | train 2.4039 | eval 2.1784 | Loss 6259.6682 | HITS@10 sub:0.0000 | HITS@10 all:0.1450\n",
      "test acc:0.1643\n",
      "Epoch 00019 | train 2.4015 | eval 2.1770 | Loss 6158.8327 | HITS@10 sub:0.0000 | HITS@10 all:0.1480\n",
      "test acc:0.1637\n",
      "Epoch 00020 | train 2.4026 | eval 2.1748 | Loss 6092.7513 | HITS@10 sub:0.0000 | HITS@10 all:0.1442\n",
      "Epoch 00021 | train 2.4010 | eval 2.1744 | Loss 6014.1922 | HITS@10 sub:0.0000 | HITS@10 all:0.1456\n",
      "Epoch 00022 | train 2.3983 | eval 2.1818 | Loss 5955.0241 | HITS@10 sub:0.0000 | HITS@10 all:0.1476\n",
      "Epoch 00023 | train 2.4037 | eval 2.1814 | Loss 5889.9650 | HITS@10 sub:0.0000 | HITS@10 all:0.1484\n",
      "test acc:0.1674\n",
      "Epoch 00024 | train 2.4054 | eval 2.1852 | Loss 5842.1237 | HITS@10 sub:0.0000 | HITS@10 all:0.1490\n",
      "test acc:0.1686\n",
      "Epoch 00025 | train 2.4045 | eval 2.1816 | Loss 5761.0105 | HITS@10 sub:0.0000 | HITS@10 all:0.1485\n",
      "Epoch 00026 | train 2.4061 | eval 2.1808 | Loss 5700.5422 | HITS@10 sub:0.0000 | HITS@10 all:0.1499\n",
      "test acc:0.1694\n",
      "Epoch 00027 | train 2.4023 | eval 2.1763 | Loss 5669.1986 | HITS@10 sub:0.0000 | HITS@10 all:0.1509\n",
      "test acc:0.1717\n",
      "Epoch 00028 | train 2.4042 | eval 2.1816 | Loss 5625.0780 | HITS@10 sub:0.0000 | HITS@10 all:0.1504\n",
      "Epoch 00029 | train 2.4056 | eval 2.1868 | Loss 5573.1555 | HITS@10 sub:0.0000 | HITS@10 all:0.1514\n",
      "test acc:0.1732\n",
      "Epoch 00030 | train 2.4020 | eval 2.1896 | Loss 5553.4502 | HITS@10 sub:0.0000 | HITS@10 all:0.1506\n",
      "Epoch 00031 | train 2.4011 | eval 2.1655 | Loss 5485.4650 | HITS@10 sub:0.0000 | HITS@10 all:0.1502\n",
      "Epoch 00032 | train 2.4030 | eval 2.1702 | Loss 5429.1861 | HITS@10 sub:0.0000 | HITS@10 all:0.1508\n",
      "Epoch 00033 | train 2.4040 | eval 2.1749 | Loss 5409.3725 | HITS@10 sub:0.0000 | HITS@10 all:0.1516\n",
      "test acc:0.1741\n",
      "Epoch 00034 | train 2.4014 | eval 2.1732 | Loss 5379.0019 | HITS@10 sub:0.0000 | HITS@10 all:0.1527\n",
      "test acc:0.1756\n",
      "Epoch 00035 | train 2.4003 | eval 2.1681 | Loss 5303.8133 | HITS@10 sub:0.0000 | HITS@10 all:0.1518\n",
      "Epoch 00036 | train 2.4008 | eval 2.1693 | Loss 5275.9488 | HITS@10 sub:0.0000 | HITS@10 all:0.1516\n",
      "Epoch 00037 | train 2.4021 | eval 2.1723 | Loss 5255.1930 | HITS@10 sub:0.0000 | HITS@10 all:0.1513\n",
      "Epoch 00038 | train 2.4033 | eval 2.1757 | Loss 5209.3069 | HITS@10 sub:0.0000 | HITS@10 all:0.1505\n",
      "Epoch 00039 | train 2.4019 | eval 2.1807 | Loss 5222.3069 | HITS@10 sub:0.0000 | HITS@10 all:0.1508\n",
      "Epoch 00040 | train 2.4006 | eval 2.1865 | Loss 5170.5815 | HITS@10 sub:0.0000 | HITS@10 all:0.1532\n",
      "test acc:0.1764\n",
      "Epoch 00041 | train 2.4015 | eval 2.1856 | Loss 5142.7560 | HITS@10 sub:0.0000 | HITS@10 all:0.1531\n",
      "Epoch 00042 | train 2.3980 | eval 2.1819 | Loss 5139.0751 | HITS@10 sub:0.0000 | HITS@10 all:0.1531\n",
      "Epoch 00043 | train 2.4003 | eval 2.2098 | Loss 5093.1077 | HITS@10 sub:0.0000 | HITS@10 all:0.1535\n",
      "test acc:0.1768\n",
      "Epoch 00044 | train 2.4330 | eval 2.1803 | Loss 5076.0728 | HITS@10 sub:0.0000 | HITS@10 all:0.1517\n",
      "Epoch 00045 | train 2.4022 | eval 2.1829 | Loss 5035.6995 | HITS@10 sub:0.0000 | HITS@10 all:0.1528\n",
      "Epoch 00046 | train 2.4065 | eval 2.1874 | Loss 5004.8947 | HITS@10 sub:0.0000 | HITS@10 all:0.1522\n",
      "Epoch 00047 | train 2.4031 | eval 2.2210 | Loss 4984.0594 | HITS@10 sub:0.0000 | HITS@10 all:0.1531\n",
      "Epoch 00048 | train 2.4039 | eval 2.2116 | Loss 4954.5730 | HITS@10 sub:0.0000 | HITS@10 all:0.1528\n",
      "Epoch 00049 | train 2.4036 | eval 2.1829 | Loss 4941.3138 | HITS@10 sub:0.0000 | HITS@10 all:0.1539\n",
      "test acc:0.1784\n",
      "Epoch 00050 | train 2.4028 | eval 2.1763 | Loss 4893.7391 | HITS@10 sub:0.0000 | HITS@10 all:0.1523\n",
      "Epoch 00051 | train 2.4003 | eval 2.1757 | Loss 4902.4727 | HITS@10 sub:0.0000 | HITS@10 all:0.1542\n",
      "test acc:0.1804\n",
      "Epoch 00052 | train 2.4628 | eval 1.7326 | Loss 4864.9446 | HITS@10 sub:0.0000 | HITS@10 all:0.1521\n",
      "Epoch 00053 | train 2.4123 | eval 1.7265 | Loss 4843.3344 | HITS@10 sub:0.0000 | HITS@10 all:0.1538\n",
      "Epoch 00054 | train 2.4074 | eval 1.7291 | Loss 4847.9162 | HITS@10 sub:0.0000 | HITS@10 all:0.1529\n",
      "Epoch 00055 | train 2.4087 | eval 1.7131 | Loss 4797.3190 | HITS@10 sub:0.0000 | HITS@10 all:0.1551\n",
      "test acc:0.1790\n",
      "Epoch 00056 | train 2.4102 | eval 1.7154 | Loss 4807.8032 | HITS@10 sub:0.0000 | HITS@10 all:0.1556\n",
      "test acc:0.1800\n",
      "Epoch 00057 | train 2.4106 | eval 1.7148 | Loss 4765.5139 | HITS@10 sub:0.0000 | HITS@10 all:0.1548\n",
      "Epoch 00058 | train 2.4049 | eval 1.7178 | Loss 4743.0431 | HITS@10 sub:0.0000 | HITS@10 all:0.1538\n",
      "Epoch 00059 | train 2.4167 | eval 1.7151 | Loss 4741.4096 | HITS@10 sub:0.0000 | HITS@10 all:0.1530\n",
      "Epoch 00060 | train 2.4122 | eval 1.7174 | Loss 4734.7811 | HITS@10 sub:0.0000 | HITS@10 all:0.1532\n",
      "Epoch 00061 | train 2.4146 | eval 1.7152 | Loss 4716.5938 | HITS@10 sub:0.0000 | HITS@10 all:0.1534\n",
      "Epoch 00062 | train 2.4100 | eval 1.7159 | Loss 4686.6583 | HITS@10 sub:0.0000 | HITS@10 all:0.1533\n",
      "Epoch 00063 | train 2.4115 | eval 1.7199 | Loss 4672.3407 | HITS@10 sub:0.0000 | HITS@10 all:0.1550\n",
      "Epoch 00064 | train 2.4095 | eval 1.7154 | Loss 4684.2942 | HITS@10 sub:0.0000 | HITS@10 all:0.1544\n",
      "Epoch 00065 | train 2.4114 | eval 1.7215 | Loss 4659.5349 | HITS@10 sub:0.0000 | HITS@10 all:0.1550\n",
      "Epoch 00066 | train 2.4097 | eval 1.7192 | Loss 4627.4587 | HITS@10 sub:0.0000 | HITS@10 all:0.1549\n",
      "Epoch 00067 | train 2.4120 | eval 1.7158 | Loss 4599.6614 | HITS@10 sub:0.0000 | HITS@10 all:0.1538\n",
      "Epoch 00068 | train 2.4064 | eval 1.7169 | Loss 4589.8984 | HITS@10 sub:0.0000 | HITS@10 all:0.1539\n",
      "Epoch 00069 | train 2.4083 | eval 1.7389 | Loss 4604.7181 | HITS@10 sub:0.0000 | HITS@10 all:0.1544\n",
      "Epoch 00070 | train 2.4094 | eval 1.7271 | Loss 4577.7452 | HITS@10 sub:0.0000 | HITS@10 all:0.1537\n",
      "Epoch 00071 | train 2.4117 | eval 1.7185 | Loss 4573.4956 | HITS@10 sub:0.0000 | HITS@10 all:0.1560\n",
      "test acc:0.1832\n",
      "Epoch 00072 | train 2.4087 | eval 1.7199 | Loss 4547.8626 | HITS@10 sub:0.0000 | HITS@10 all:0.1567\n",
      "test acc:0.1828\n",
      "Epoch 00073 | train 2.4132 | eval 1.7188 | Loss 4538.2813 | HITS@10 sub:0.0000 | HITS@10 all:0.1561\n",
      "Epoch 00074 | train 2.4085 | eval 1.7316 | Loss 4547.0293 | HITS@10 sub:0.0000 | HITS@10 all:0.1562\n",
      "Epoch 00075 | train 2.4105 | eval 1.7211 | Loss 4492.9659 | HITS@10 sub:0.0000 | HITS@10 all:0.1543\n",
      "Epoch 00076 | train 2.4089 | eval 1.7169 | Loss 4489.5529 | HITS@10 sub:0.0000 | HITS@10 all:0.1540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00077 | train 2.4123 | eval 1.7187 | Loss 4489.7421 | HITS@10 sub:0.0000 | HITS@10 all:0.1551\n",
      "Epoch 00078 | train 2.4064 | eval 1.7177 | Loss 4479.5447 | HITS@10 sub:0.0000 | HITS@10 all:0.1549\n",
      "Epoch 00079 | train 2.4100 | eval 1.7192 | Loss 4463.2181 | HITS@10 sub:0.0000 | HITS@10 all:0.1538\n",
      "Epoch 00080 | train 2.4073 | eval 1.7190 | Loss 4446.2377 | HITS@10 sub:0.0000 | HITS@10 all:0.1545\n",
      "Epoch 00081 | train 2.4098 | eval 1.7212 | Loss 4433.9150 | HITS@10 sub:0.0000 | HITS@10 all:0.1548\n",
      "Epoch 00082 | train 2.4092 | eval 1.7176 | Loss 4436.8742 | HITS@10 sub:0.0000 | HITS@10 all:0.1558\n",
      "Epoch 00083 | train 2.4110 | eval 1.7162 | Loss 4397.6203 | HITS@10 sub:0.0000 | HITS@10 all:0.1554\n",
      "Epoch 00084 | train 2.4090 | eval 1.7173 | Loss 4403.2628 | HITS@10 sub:0.0000 | HITS@10 all:0.1548\n",
      "Epoch 00085 | train 2.4083 | eval 1.7181 | Loss 4394.7864 | HITS@10 sub:0.0000 | HITS@10 all:0.1557\n",
      "Epoch 00086 | train 2.4065 | eval 1.7210 | Loss 4398.4028 | HITS@10 sub:0.0000 | HITS@10 all:0.1533\n",
      "Epoch 00087 | train 2.4115 | eval 1.7169 | Loss 4376.2922 | HITS@10 sub:0.0000 | HITS@10 all:0.1541\n",
      "Epoch 00088 | train 2.4100 | eval 1.7154 | Loss 4398.3672 | HITS@10 sub:0.0000 | HITS@10 all:0.1547\n",
      "Epoch 00089 | train 2.4121 | eval 1.7177 | Loss 4363.1228 | HITS@10 sub:0.0000 | HITS@10 all:0.1558\n",
      "Epoch 00090 | train 2.4077 | eval 1.7179 | Loss 4335.0321 | HITS@10 sub:0.0000 | HITS@10 all:0.1543\n",
      "Epoch 00091 | train 2.4114 | eval 1.7154 | Loss 4345.2632 | HITS@10 sub:0.0000 | HITS@10 all:0.1553\n",
      "Epoch 00092 | train 2.4080 | eval 1.7154 | Loss 4305.6264 | HITS@10 sub:0.0000 | HITS@10 all:0.1539\n",
      "Epoch 00093 | train 2.4142 | eval 1.7150 | Loss 4316.3415 | HITS@10 sub:0.0000 | HITS@10 all:0.1535\n",
      "Epoch 00094 | train 2.4120 | eval 1.7170 | Loss 4300.6604 | HITS@10 sub:0.0000 | HITS@10 all:0.1545\n",
      "Epoch 00095 | train 2.4130 | eval 1.7193 | Loss 4303.5515 | HITS@10 sub:0.0000 | HITS@10 all:0.1542\n",
      "Epoch 00096 | train 2.4084 | eval 1.7176 | Loss 4288.4734 | HITS@10 sub:0.0000 | HITS@10 all:0.1539\n",
      "Epoch 00097 | train 2.4112 | eval 1.7154 | Loss 4276.3884 | HITS@10 sub:0.0000 | HITS@10 all:0.1549\n",
      "Epoch 00098 | train 2.4069 | eval 1.7150 | Loss 4269.4349 | HITS@10 sub:0.0000 | HITS@10 all:0.1545\n",
      "Epoch 00099 | train 2.4124 | eval 1.7154 | Loss 4248.1632 | HITS@10 sub:0.0000 | HITS@10 all:0.1540\n",
      "Epoch 00100 | train 2.4095 | eval 1.7166 | Loss 4237.6492 | HITS@10 sub:0.0000 | HITS@10 all:0.1540\n",
      "Epoch 00101 | train 2.4113 | eval 1.7300 | Loss 4249.2837 | HITS@10 sub:0.0000 | HITS@10 all:0.1548\n",
      "Epoch 00102 | train 2.4050 | eval 1.7143 | Loss 4228.8486 | HITS@10 sub:0.0000 | HITS@10 all:0.1550\n",
      "Epoch 00103 | train 2.4093 | eval 1.7185 | Loss 4243.8872 | HITS@10 sub:0.0000 | HITS@10 all:0.1547\n",
      "Epoch 00104 | train 2.4053 | eval 1.7178 | Loss 4218.9117 | HITS@10 sub:0.0000 | HITS@10 all:0.1546\n",
      "Epoch 00105 | train 2.4102 | eval 1.7166 | Loss 4214.5731 | HITS@10 sub:0.0000 | HITS@10 all:0.1541\n",
      "Epoch 00106 | train 2.4067 | eval 1.7143 | Loss 4237.1165 | HITS@10 sub:0.0000 | HITS@10 all:0.1536\n",
      "Epoch 00107 | train 2.4113 | eval 1.7206 | Loss 4177.3633 | HITS@10 sub:0.0000 | HITS@10 all:0.1545\n",
      "Epoch 00108 | train 2.4090 | eval 1.7175 | Loss 4196.3385 | HITS@10 sub:0.0000 | HITS@10 all:0.1533\n",
      "Epoch 00109 | train 2.4113 | eval 1.7219 | Loss 4183.1498 | HITS@10 sub:0.0000 | HITS@10 all:0.1547\n",
      "Epoch 00110 | train 2.4110 | eval 1.7196 | Loss 4185.4802 | HITS@10 sub:0.0000 | HITS@10 all:0.1543\n",
      "Epoch 00111 | train 2.4121 | eval 1.7170 | Loss 4146.0934 | HITS@10 sub:0.0000 | HITS@10 all:0.1535\n",
      "Epoch 00112 | train 2.4077 | eval 1.7205 | Loss 4166.4774 | HITS@10 sub:0.0000 | HITS@10 all:0.1532\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0162d3287db1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mnegs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI_U\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_U\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mEdgeSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_item_spm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_sample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI_U\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_U\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-f101c1245fef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, I, U, I_neg, true_neg, I_U, N_U)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI_U\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_U\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfism_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI_U\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_U\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m#neg_sample_size = int(len(r_neg) / len(r))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#r_neg = r_neg.reshape((-1, neg_sample_size))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-098280413368>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, I, U, I_neg, I_U, N_U, test)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI_U\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m# If this is training, we need to subtract the embedding of the self node from the context embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-f101c1245fef>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(I)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgconv_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgconv_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgconv_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgconv_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgconv_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgconv_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfism_rating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFISMrating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_movies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-97deb8399eea>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, features)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_edge_similarity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'similarity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dgl-tutorial-full/advanced_apps/rec/sageconv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, graph, feat, e_feat)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mis\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mh_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dgl/python/dgl/graph.py\u001b[0m in \u001b[0;36mlocal_var\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3424\u001b[0m         return DGLGraph(self._graph,\n\u001b[1;32m   3425\u001b[0m                         \u001b[0mlocal_node_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3426\u001b[0;31m                         local_edge_frame)\n\u001b[0m\u001b[1;32m   3427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dgl/python/dgl/graph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph_data, node_frame, edge_frame, multigraph, readonly, sort_csr)\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edge_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edge_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;31m# message indicator:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;31m# if self._msg_index[eid] == 1, then edge eid has message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "#Model hyperparameters\n",
    "n_hidden = 256\n",
    "n_layers = 1\n",
    "dropout = 0.4\n",
    "aggregator_type = 'sum' if use_edge_similarity else 'gcn'\n",
    "\n",
    "# create GraphSAGE model\n",
    "gconv_p = GraphSAGEModel(n_hidden,\n",
    "                         n_hidden,\n",
    "                         n_hidden,\n",
    "                         n_layers,\n",
    "                         F.relu,\n",
    "                         dropout,\n",
    "                         aggregator_type)\n",
    "\n",
    "gconv_q = GraphSAGEModel(n_hidden,\n",
    "                         n_hidden,\n",
    "                         n_hidden,\n",
    "                         n_layers,\n",
    "                         F.relu,\n",
    "                         dropout,\n",
    "                         aggregator_type)\n",
    "\n",
    "model = FISM(user_item_spm, gconv_p, gconv_q, g, n_hidden, device).to(device)\n",
    "g.to(device)\n",
    "\n",
    "# Training hyperparameters\n",
    "weight_decay = 1e-5\n",
    "n_epochs = 200\n",
    "lr = 1e-3\n",
    "neg_sample_size = 20\n",
    "\n",
    "# use optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "batch_size = 1024\n",
    "print('#edges:', user_item_spm.nnz)\n",
    "print('#batch/epoch:', user_item_spm.nnz/batch_size)\n",
    "\n",
    "# initialize graph\n",
    "dur = []\n",
    "best_acc = 0\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    start = time.time()\n",
    "    negs = []\n",
    "    for I, U, I_neg, true_neg, I_U, N_U in EdgeSampler(user_item_spm, batch_size, neg_sample_size):\n",
    "        loss = model(I, U, I_neg, true_neg, I_U, N_U)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach().item())\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    start = time.time()\n",
    "    hits10_sub = 0\n",
    "    hits_all = RecEvalAll(model, user_item_spm, 10, 'valid')\n",
    "    eval_time = time.time() - start\n",
    "    print(\"Epoch {:05d} | train {:.4f} | eval {:.4f} | Loss {:.4f} | HITS@10 sub:{:.4f} | HITS@10 all:{:.4f}\".format(\n",
    "        epoch, train_time, eval_time, np.mean(losses), hits10_sub, hits_all))\n",
    "    if best_acc < hits_all:\n",
    "        best_acc = hits_all\n",
    "        test_hits_all = RecEvalAll(model, user_item_spm, 10, 'test')\n",
    "        print('test acc:{:.4f}'.format(test_hits_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
