{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "\n",
    "# Load Pytorch as backend\n",
    "dgl.load_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy import sparse as spsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN models from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sageconv import SAGEConv\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 out_dim,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 aggregator_type):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # input layer\n",
    "        self.layers.append(SAGEConv(in_feats, n_hidden, aggregator_type,\n",
    "                                    feat_drop=dropout, activation=activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(SAGEConv(n_hidden, n_hidden, aggregator_type,\n",
    "                                        feat_drop=dropout, activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(SAGEConv(n_hidden, out_dim, aggregator_type,\n",
    "                                    feat_drop=dropout, activation=None))\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h, g.edata['similarity'])\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "#stanfordnlp.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from movielens import MovieLens\n",
    "data = MovieLens('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate some statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = data.ratings\n",
    "\n",
    "user_id = np.array(ratings['user_idx'])\n",
    "movie_id = np.array(ratings['movie_idx'])\n",
    "print('#user-movie:', len(movie_id))\n",
    "user_movie_spm = spsp.coo_matrix((np.ones((len(user_id),)), (user_id, movie_id)))\n",
    "num_users, num_movies = user_movie_spm.shape\n",
    "print('#users:', num_users)\n",
    "print('#movies:', num_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into a training set and a testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = ratings[~(ratings['valid_mask'] | ratings['test_mask'])]\n",
    "user_latest_item_indices = (\n",
    "        ratings_train.groupby('user_id')['timestamp'].transform(pd.Series.max) ==\n",
    "        ratings_train['timestamp'])\n",
    "user_latest_item = ratings_train[user_latest_item_indices]\n",
    "user_latest_item = dict(\n",
    "        zip(user_latest_item['user_idx'].values, user_latest_item['movie_idx'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = np.array(ratings_train['user_idx'])\n",
    "movie_id = np.array(ratings_train['movie_idx'])\n",
    "user_movie_spm = spsp.coo_matrix((np.ones((len(user_id),)), (user_id, movie_id)))\n",
    "assert num_users == user_movie_spm.shape[0]\n",
    "assert num_movies == user_movie_spm.shape[1]\n",
    "train_size = len(user_id)\n",
    "print('#training size:', train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the validation and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_valid = ratings[ratings['valid_mask']]['user_idx'].values\n",
    "movies_valid = ratings[ratings['valid_mask']]['movie_idx'].values\n",
    "users_test = ratings[ratings['test_mask']]['user_idx'].values\n",
    "movies_test = ratings[ratings['test_mask']]['movie_idx'].values\n",
    "valid_size = len(users_valid)\n",
    "test_size = len(users_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the movie graph with SLIM\n",
    "Here is another way of constructing a moive matrix by taking advantage of SLIM. SLIM is an item-based recommendation model. When training SLIM on a user-movie dataset, it learns a movie similarity matrix. This similarity matrix is the movie matrix we like to construct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from SLIM import SLIM, SLIMatrix\n",
    "#model = SLIM()\n",
    "#params = {'algo': 'cd', 'nthreads': 2, 'l1r': 1.0, 'l2r': 1.0}\n",
    "#trainmat = SLIMatrix(user_movie_spm.tocsr())\n",
    "#model.train(params, trainmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_model(modelfname='slim_model.csr', mapfname='slim_map.csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csr(filename):\n",
    "    f = open(filename, 'r')\n",
    "    all_rows = []\n",
    "    all_cols = []\n",
    "    all_vals = []\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        strs = line.split(' ')\n",
    "        cols = [int(s) for s in strs[1::2]]\n",
    "        vals = [float(s) for s in strs[2::2]]\n",
    "        all_cols.extend(cols)\n",
    "        all_vals.extend(vals)\n",
    "        all_rows.extend([i for _ in cols])\n",
    "    all_rows = np.array(all_rows, dtype=np.int64)\n",
    "    all_cols = np.array(all_cols, dtype=np.int64)\n",
    "    all_vals = np.array(all_vals, dtype=np.float32)\n",
    "    mat = spsp.coo_matrix((all_vals, (all_rows, all_cols)))\n",
    "    return mat\n",
    "\n",
    "movie_spm = read_csr('slim_model.csr')\n",
    "assert movie_spm.shape[0] == user_movie_spm.shape[1]\n",
    "assert movie_spm.shape[1] == user_movie_spm.shape[1]\n",
    "print('#edges:', movie_spm.nnz)\n",
    "print('most similar:', np.max(movie_spm.data))\n",
    "print('most unsimilar:', np.min(movie_spm.data))\n",
    "print('#similar:', np.sum(movie_spm > 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.DGLGraph(movie_spm, readonly=True)\n",
    "g.edata['similarity'] = torch.tensor(movie_spm.data, dtype=torch.float32)\n",
    "year = np.expand_dims(data.movie_data['year'], axis=1)\n",
    "genre = data.movie_data['genre']\n",
    "print('#genre:', genre.shape[1])\n",
    "title = data.movie_data['title']\n",
    "print('title vocabulary:', title.shape[1])\n",
    "features = torch.tensor(np.concatenate((genre, title), axis=1), dtype=torch.float32)\n",
    "#features = genre\n",
    "print('#movies:', g.number_of_nodes())\n",
    "print('#edges:', g.number_of_edges())\n",
    "print('#features:', features.shape[1])\n",
    "in_feats = features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-based recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeLayer(nn.Module):\n",
    "    def __init__(self, in_feats, num_hidden):\n",
    "        super(EncodeLayer, self).__init__()\n",
    "        self.proj = nn.Linear(in_feats, num_hidden)\n",
    "        \n",
    "    def forward(self, feats):\n",
    "        return self.proj(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "n_hidden = 64\n",
    "n_layers = 1\n",
    "dropout = 0.5\n",
    "aggregator_type = 'sum'\n",
    "\n",
    "# create GraphSAGE model\n",
    "gconv_model = GraphSAGEModel(n_hidden,\n",
    "                             n_hidden,\n",
    "                             n_hidden,\n",
    "                             n_layers,\n",
    "                             F.relu,\n",
    "                             dropout,\n",
    "                             aggregator_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCE loss\n",
    "def NCE_loss(pos_score, neg_score, neg_sample_size):\n",
    "    pos_score = F.logsigmoid(pos_score)\n",
    "    neg_score = F.logsigmoid(-neg_score).reshape(-1, neg_sample_size)\n",
    "    return -pos_score - torch.sum(neg_score, dim=1)\n",
    "\n",
    "class LinkPrediction(nn.Module):\n",
    "    def __init__(self, gconv_model):\n",
    "        super(LinkPrediction, self).__init__()\n",
    "        self.encode = EncodeLayer(in_feats, n_hidden)\n",
    "        self.gconv_model = gconv_model\n",
    "\n",
    "    def forward(self, g, features, neg_sample_size):\n",
    "        emb = self.encode(features)\n",
    "        emb = self.gconv_model(g, emb)\n",
    "        #emb = self.gconv_model(g, features)\n",
    "        pos_g, neg_g = edge_sampler(g, neg_sample_size, return_false_neg=False)\n",
    "        pos_score = score_func(pos_g, emb)\n",
    "        neg_score = score_func(neg_g, emb)\n",
    "        return torch.mean(NCE_loss(pos_score, neg_score, neg_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_sampler(g, neg_sample_size, edges=None, return_false_neg=True):\n",
    "    sampler = dgl.contrib.sampling.EdgeSampler(g, batch_size=int(g.number_of_edges()/10),\n",
    "                                               seed_edges=edges,\n",
    "                                               neg_sample_size=neg_sample_size,\n",
    "                                               negative_mode='tail',\n",
    "                                               shuffle=True,\n",
    "                                               return_false_neg=return_false_neg)\n",
    "    sampler = iter(sampler)\n",
    "    return next(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func(g, emb):\n",
    "    src_nid, dst_nid = g.all_edges(order='eid')\n",
    "    # Get the node Ids in the parent graph.\n",
    "    src_nid = g.parent_nid[src_nid]\n",
    "    dst_nid = g.parent_nid[dst_nid]\n",
    "    # Read the node embeddings of the source nodes and destination nodes.\n",
    "    pos_heads = emb[src_nid]\n",
    "    pos_tails = emb[dst_nid]\n",
    "    # cosine similarity\n",
    "    return torch.sum(pos_heads * pos_tails, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LPEvaluate(model, g, features, users_eval, movies_eval, neg_sample_size):\n",
    "    gconv_model.eval()\n",
    "    with torch.no_grad():\n",
    "        emb = model.encode(features)\n",
    "        emb = model.gconv_model(g, emb)\n",
    "        loss = model(g, features, neg_sample_size)\n",
    "        #emb = model.gconv_model(g, features)\n",
    "        hits_10s = []\n",
    "        # evaluate one user-item interaction at a time\n",
    "        for u, i in zip(users_eval, movies_eval):\n",
    "            I_q = user_latest_item[u]\n",
    "            I = torch.cat([torch.LongTensor([i]), torch.LongTensor(data.neg_valid[u])])\n",
    "            Z_q = emb[I_q]\n",
    "            Z = emb[I]\n",
    "            score = (Z_q[None, :] * Z).sum(1).cpu().numpy()\n",
    "            rank = stats.rankdata(-score, 'min')\n",
    "            hits_10s.append(rank[0] <= 10)\n",
    "        print('HITS@10:{:.4f}, loss:{:.4f}'.format(np.mean(hits_10s), loss.item()))\n",
    "        return np.mean(hits_10s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model for link prediction\n",
    "model = LinkPrediction(gconv_model)\n",
    "\n",
    "# Training hyperparameters\n",
    "weight_decay = 5e-4\n",
    "n_epochs = 20\n",
    "lr = 1e-3\n",
    "neg_sample_size = 40\n",
    "\n",
    "# use optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# initialize graph\n",
    "dur = []\n",
    "prev_acc = 0\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    loss = model(g, features, neg_sample_size)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Epoch {:05d} | Loss {:.4f}\".format(epoch, loss.item()))\n",
    "    \n",
    "    acc = LPEvaluate(model, g, features, users_valid, movies_valid, neg_sample_size)\n",
    "    if epoch > 5 and acc <= prev_acc:\n",
    "        break\n",
    "    prev_acc = acc\n",
    "\n",
    "print()\n",
    "# Let's save the trained node embeddings.\n",
    "LPEvaluate(model, g, features, users_test, movies_test, neg_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
