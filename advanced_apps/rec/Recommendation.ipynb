{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "\n",
    "# Load Pytorch as backend\n",
    "dgl.load_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse as spsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import conv as dgl_conv\n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 out_dim,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 aggregator_type):\n",
    "        super(GraphSAGEModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # input layer\n",
    "        self.layers.append(dgl_conv.SAGEConv(in_feats, n_hidden, aggregator_type,\n",
    "                                         feat_drop=dropout, activation=activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(dgl_conv.SAGEConv(n_hidden, n_hidden, aggregator_type,\n",
    "                                             feat_drop=dropout, activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(dgl_conv.SAGEConv(n_hidden, out_dim, aggregator_type,\n",
    "                                         feat_drop=dropout, activation=None))\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "stanfordnlp.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from movielens import MovieLens\n",
    "data = MovieLens('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = np.array(data.ratings['user_idx'])\n",
    "movie_id = np.array(data.ratings['movie_idx'])\n",
    "print('#user-movie:', len(movie_id))\n",
    "spm = spsp.coo_matrix((np.ones((len(user_id),)), (user_id, movie_id)))\n",
    "print(spm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(np.unique(user_id))\n",
    "spm_t = spm.transpose()\n",
    "movie_deg = spm_t.dot(np.ones((num_users,)))\n",
    "movie_ratio = movie_deg / np.sum(movie_deg)\n",
    "# 1e-6 is a hyperparameter for this dataset.\n",
    "movie_sample_prob = 1 - np.maximum(1 - np.sqrt(1e-6 / movie_ratio), 0)\n",
    "sample_prob = movie_sample_prob[movie_id]\n",
    "sample = np.random.uniform(size=(len(movie_id),))\n",
    "user_id = user_id[sample_prob > sample]\n",
    "movie_id = movie_id[sample_prob > sample]\n",
    "print('#samples:', len(user_id))\n",
    "spm = spsp.coo_matrix((np.ones((len(user_id),)), (user_id, movie_id)))\n",
    "print(spm.shape)\n",
    "movie_deg = spm_t.dot(np.ones((num_users,)))\n",
    "print(np.sum(movie_deg == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_spm = np.dot(spm.transpose(), spm)\n",
    "dense_movie = np.sort(movie_spm.todense())\n",
    "topk_movie = dense_movie[:,-50]\n",
    "topk_movie_spm = movie_spm >= topk_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.DGLGraph(topk_movie_spm, readonly=True)\n",
    "year = np.expand_dims(data.movie_data['year'], axis=1)\n",
    "genre = data.movie_data['genre']\n",
    "print('#genre:', genre.shape[1])\n",
    "title = data.movie_data['title']\n",
    "print('title vocabulary:', title.shape[1])\n",
    "features = torch.tensor(np.concatenate((genre, title), axis=1), dtype=torch.float32)\n",
    "#features = genre\n",
    "print('#movies:', g.number_of_nodes())\n",
    "print('#edges:', g.number_of_edges())\n",
    "print('#features:', features.shape[1])\n",
    "in_feats = features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeLayer(nn.Module):\n",
    "    def __init__(self, in_feats, num_hidden):\n",
    "        super(EncodeLayer, self).__init__()\n",
    "        self.proj = nn.Linear(in_feats, num_hidden)\n",
    "        \n",
    "    def forward(self, feats):\n",
    "        return self.proj(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "n_hidden = 64\n",
    "n_layers = 1\n",
    "dropout = 0.3\n",
    "aggregator_type = 'gcn'\n",
    "\n",
    "# create GraphSAGE model\n",
    "gconv_model = GraphSAGEModel(n_hidden,\n",
    "                             n_hidden,\n",
    "                             n_hidden,\n",
    "                             n_layers,\n",
    "                             F.relu,\n",
    "                             dropout,\n",
    "                             aggregator_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCE loss\n",
    "def NCE_loss(pos_score, neg_score, neg_sample_size):\n",
    "    pos_score = F.logsigmoid(pos_score)\n",
    "    neg_score = F.logsigmoid(-neg_score).reshape(-1, neg_sample_size)\n",
    "    return -pos_score - torch.sum(neg_score, dim=1)\n",
    "\n",
    "class LinkPrediction(nn.Module):\n",
    "    def __init__(self, gconv_model):\n",
    "        super(LinkPrediction, self).__init__()\n",
    "        self.encode = EncodeLayer(in_feats, n_hidden)\n",
    "        self.gconv_model = gconv_model\n",
    "\n",
    "    def forward(self, g, features, neg_sample_size):\n",
    "        emb = self.encode(features)\n",
    "        emb = self.gconv_model(g, emb)\n",
    "        #emb = self.gconv_model(g, features)\n",
    "        pos_g, neg_g = edge_sampler(g, neg_sample_size, return_false_neg=False)\n",
    "        pos_score = score_func(pos_g, emb)\n",
    "        neg_score = score_func(neg_g, emb)\n",
    "        return torch.mean(NCE_loss(pos_score, neg_score, neg_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_sampler(g, neg_sample_size, edges=None, return_false_neg=True):\n",
    "    sampler = dgl.contrib.sampling.EdgeSampler(g, batch_size=int(g.number_of_edges()/10),\n",
    "                                               seed_edges=edges,\n",
    "                                               neg_sample_size=neg_sample_size,\n",
    "                                               negative_mode='tail',\n",
    "                                               shuffle=True,\n",
    "                                               return_false_neg=return_false_neg)\n",
    "    sampler = iter(sampler)\n",
    "    return next(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func(g, emb):\n",
    "    src_nid, dst_nid = g.all_edges(order='eid')\n",
    "    # Get the node Ids in the parent graph.\n",
    "    src_nid = g.parent_nid[src_nid]\n",
    "    dst_nid = g.parent_nid[dst_nid]\n",
    "    # Read the node embeddings of the source nodes and destination nodes.\n",
    "    pos_heads = emb[src_nid]\n",
    "    pos_tails = emb[dst_nid]\n",
    "    # cosine similarity\n",
    "    return torch.sum(pos_heads * pos_tails, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LPEvaluate(model, g, features, eval_eids, neg_sample_size):\n",
    "    gconv_model.eval()\n",
    "    with torch.no_grad():\n",
    "        emb = model.encode(features)\n",
    "        emb = model.gconv_model(g, emb)\n",
    "        #emb = model.gconv_model(g, features)\n",
    "        \n",
    "        pos_g, neg_g = edge_sampler(g, neg_sample_size, eval_eids, return_false_neg=True)\n",
    "        pos_score = score_func(pos_g, emb)\n",
    "        neg_score = score_func(neg_g, emb).reshape(-1, neg_sample_size)\n",
    "        filter_bias = neg_g.edata['false_neg'].reshape(-1, neg_sample_size)\n",
    "\n",
    "        pos_score = F.logsigmoid(pos_score)\n",
    "        neg_score = F.logsigmoid(neg_score)\n",
    "        neg_score -= filter_bias.float()\n",
    "        pos_score = pos_score.unsqueeze(1)\n",
    "        rankings = torch.sum(neg_score >= pos_score, dim=1) + 1\n",
    "        return np.mean(1.0/rankings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eids = np.random.permutation(g.number_of_edges())\n",
    "train_eids = eids[:int(len(eids) * 0.8)]\n",
    "valid_eids = eids[int(len(eids) * 0.8):int(len(eids) * 0.9)]\n",
    "test_eids = eids[int(len(eids) * 0.9):]\n",
    "train_g = g.edge_subgraph(train_eids, preserve_nodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model for link prediction\n",
    "model = LinkPrediction(gconv_model)\n",
    "\n",
    "# Training hyperparameters\n",
    "weight_decay = 5e-4\n",
    "n_epochs = 200\n",
    "lr = 1e-3\n",
    "neg_sample_size = 10\n",
    "\n",
    "# use optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# initialize graph\n",
    "dur = []\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    loss = model(train_g, features, neg_sample_size)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc = LPEvaluate(model, g, features, valid_eids, neg_sample_size)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | MRR {:.4f}\".format(epoch, loss.item(), acc))\n",
    "\n",
    "print()\n",
    "# Let's save the trained node embeddings.\n",
    "acc = LPEvaluate(model, g, features, test_eids, neg_sample_size)\n",
    "print(\"Test MRR {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
